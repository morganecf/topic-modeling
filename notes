Supervised LDA:

In supervised latent Dirichlet allocation (sLDA), we add to LDA a response variable associated
with each document. As mentioned, this variable might be the number of stars given to a movie, a
count of the users in an on-line community who marked an article interesting, or the category of a
document. We jointly model the documents and the responses, in order to find latent topics that will
best predict the response variables for future unlabeled documents.

Neural networks: use a NN to calculate the posterior.

Variational autoencoders:

Neural style transfer: use 2 word2vec models, one for each author.
"Translate" a model from one author to another by taking each context from author1 and using model2 to
predict word author2 would have used in that context. This gives you sentence structure from author1,
but words from author2.

Tools:
- Stanford CoreNLP: http://stanfordnlp.github.io/CoreNLP/
- Mr.LDA (scalable)
- gensim (python)
- MALLET (java): http://programminghistorian.org/lessons/topic-modeling-and-mallet

Very good topic modeling reading list: http://www.biasedestimates.com/p/topic-models-reading-list.html