Some basic questions:
  Which domains do I visit the most?
  Which domains do I spend the most time on?
  Which domains do I access directly (as opposed to through google?)
  For heavy-usage domains, what are the top subdomains? Who do I message the most?
  Which categories do I spend the most time on? (messaging, email, googling, videos, work/code, news)
  How do I use google? (search terms)
  How much time do I spend per day on different categories?
  What does weekend vs. weekly usage look like? What does morning vs. afternoon vs. night usage look like?
  What are the most common n-grams for url titles?
  How does all this change over time?
  What are the most common n-grams for search terms?
  When do I watch netflix?
  When do I message?
  When do I email?
More advanced:
  What are the most common n-grams (websites visited sequentially)? Can we model frequency of opening up messaging apps like messenger/facebook/slack?
  Which topics are present? Which topics do I spend the most time on? How do topics change over time?
    Using LDA
    Using more primitive dimensionality reduction techniques
  Can we predict when I'll visit a certain website? (use history features)


History features: statistics on gap between events (page or subdomain visit):
  Event count over history range (1W, 4W, 16W)
  Average time gap beween events
  Median time gap between events
  Standard dev. of time gap between events
  25% percentile of time gaps between events
  75% percentile of time gaps between events
  10% percentile of time gaps between events
  90% percentile of time gaps between events
  Time gap between start and last event date
  Time gap between start and average event date
  Last value, second to last value, third to last value
  Sum
  Average
  Median
  Standard dev
  Mean absolute dev
  Min
  Max
  25% percentile
  75% percentile
  10% percentile
  90% percentile
  Slope
  Most frequent value
  Entropy
  Sequences
  ....All should be computed over a history range (1W, 4W, 16W, or other)
To reduce the number of features, compute feature impact on xgboost trained on a subsample of the data, and keep top N features
  default = 50

>> Events should be things like messenger/slack/google/github visit

Note: current browser history starts at May 17th, 2016


TODO
- visualize with seaborn or something
- visualize with vega